"""
Core learner classes for the meditation simulation framework.

This module contains the main learner implementations:
    - RuleBasedLearner: Foundation class with core thoughtseed dynamics
    - ActInfLearner: Active Inference extension with network modeling
    
All methods are preserved exactly from the original implementation to maintain
complete compatibility and functionality.
"""

import numpy as np
import pickle
import matplotlib.pyplot as plt
import os
import json
from collections import defaultdict

from config import (
    THOUGHTSEEDS, STATES, STATE_DWELL_TIMES, THOUGHTSEED_AGENTS, 
    NETWORK_PROFILES, ActiveInferenceConfig, ThoughtseedParams, MetacognitionParams
)
from utils import ensure_directories, _save_json_outputs


class RuleBasedLearner:
    """
    Foundation class implementing rule-based thoughtseed dynamics.
    
    This class provides core methods for thoughtseed behavior, meta-awareness,
    and state transitions, serving as a foundation for the active inference implementation.
    It models meditation dynamics using rule-based interactions between thoughtseeds.
    """
    
    def __init__(self, experience_level='novice', timesteps_per_cycle=200):

        # Core parameters
        self.experience_level = experience_level
        self.timesteps = timesteps_per_cycle
        self.thoughtseeds = THOUGHTSEEDS
        self.states = STATES
        self.num_thoughtseeds = len(self.thoughtseeds)
        
        # State tracking
        self.state_indices = {state: i for i, state in enumerate(self.states)}
        self.transition_counts = defaultdict(lambda: defaultdict(int))
        self.natural_transition_count = 0
        self.forced_transition_count = 0
        
        # History tracking
        self.activations_history = []
        self.state_history = []
        self.meta_awareness_history = []
        self.dominant_ts_history = []
        self.state_history_over_time = []
        
        # Get noise level from config
        aif_params = ActiveInferenceConfig.get_params(experience_level)
        self.noise_level = aif_params['noise_level']
        
        # Track activation patterns at transition points
        self.transition_activations = {state: [] for state in self.states}
        
        # Track distraction buildup patterns
        self.distraction_buildup_rates = []

    def get_target_activations(self, state, meta_awareness):
        """
        Generate target activations for each thoughtseed based on state and meta-awareness.        
        This determines the ideal activation pattern for each state,
        which is then modulated by neural network dynamics.
        
        """
        # Get target activations from the parameter class
        targets_dict = ThoughtseedParams.get_target_activations(
            state, meta_awareness, self.experience_level)
        
        # Convert dictionary to numpy array in the correct order
        target_activations = np.zeros(self.num_thoughtseeds)
        for i, ts in enumerate(self.thoughtseeds):
            target_activations[i] = targets_dict[ts]
        
        # Add noise for biological plausibility
        target_activations += np.random.normal(0, self.noise_level, size=self.num_thoughtseeds)
        
        # Ensure values are in proper range
        return np.clip(target_activations, 0.05, 1.0)

    def get_dwell_time(self, state):
        """
        Generate a random dwell time for the given state, based on experience level.

        """
        # Get the configured range from STATE_DWELL_TIMES
        config_min, config_max = STATE_DWELL_TIMES[self.experience_level][state]
        
        # Ensure minimal biological plausibility while respecting configured values
        if state in ['meta_awareness', 'redirect_breath']:
            # For brief states: at least 1 timestep, respect configured max
            min_biological = 1
            max_biological = config_max
        else:
            # For longer states: at least 3 timesteps, respect configured max
            min_biological = 3
            max_biological = config_max
        
        # Generate dwell time with proper constraints
        return max(min_biological, min(max_biological, np.random.randint(config_min, config_max + 1)))

    def get_meta_awareness(self, current_state, activations):
        """
        Calculate meta-awareness based on state and thoughtseed activations.
        
        Meta-awareness is higher in experts, higher during meta_awareness state,
        and influenced by self-reflection and equanimity thoughtseeds.
        """
        # Convert activations array to dictionary for parameter class
        activations_dict = {}
        for i, ts in enumerate(self.thoughtseeds):
            activations_dict[ts] = activations[i]
        
        # Get meta-awareness from parameter class
        meta_awareness = MetacognitionParams.calculate_meta_awareness(
            current_state, activations_dict, self.experience_level)
        
        # Add small random noise for variability
        meta_awareness += np.random.normal(0, 0.05)
        
        return np.clip(meta_awareness, 0.2, 0.85 if self.experience_level == 'novice' else 0.9)


class ActInfLearner(RuleBasedLearner):
    """
    Active Inference extension of the RuleBasedLearner, implementing the three-level
    Thoughtseeds Framework from the paper:
    
    Level 1: Attentional Network Superordinate Ensembles (DMN, VAN, DAN, FPN)
    Level 2: Thoughtseed Dynamics (breath_focus, pending_tasks, etc.)
    Level 3: Metacognitive Regulation (meta-awareness, precision weighting)
    
    This implementation models the four-stage Vipassana cycle:
    1. Breath Control (sustained attention)
    2. Mind Wandering (attentional drift)
    3. Meta-Awareness (noticing the lapse)
    4. Redirect Attention (returning to breath)
    
    The model evolves through active inference by minimizing variational free energy,
    implementing Equations 1-4 from the theoretical framework.
    """
    def __init__(self, experience_level='novice', timesteps_per_cycle=200):
        # Initialize base class
        super().__init__(experience_level, timesteps_per_cycle)
        
        # Initialize network-specific parameters
        self.networks = ['DMN', 'VAN', 'DAN', 'FPN']
        self.network_activations_history = []
        self.free_energy_history = []
        self.prediction_error_history = []
        self.precision_history = []
        
        # Get all active inference parameters from centralized config
        aif_params = ActiveInferenceConfig.get_params(experience_level)
        
        # Unpack parameters
        self.precision_weight = aif_params['precision_weight']
        self.complexity_penalty = aif_params['complexity_penalty']
        self.learning_rate = aif_params['learning_rate']
        self.noise_level = aif_params['noise_level']
        self.memory_factor = aif_params['memory_factor']
        self.fpn_enhancement = aif_params['fpn_enhancement']
        self.transition_thresholds = aif_params['transition_thresholds']
        self.network_modulation = aif_params['network_modulation']
        self.non_linear_dynamics = self.network_modulation.get('non_linear_dynamics', {})
        
        # Track learned network profiles
        self.learned_network_profiles = {
            "thoughtseed_contributions": {ts: {} for ts in self.thoughtseeds},
            "state_network_expectations": {state: {} for state in self.states}
        }
        
        # Initialize tracking variables used in train()
        self.distraction_buildup_rates = []
        self.transition_activations = {state: [] for state in self.states}
        
        self.in_transition = False
        self.transition_counter = 0
        self.transition_target = None
        
        # Initialize with default profiles
        for ts in self.thoughtseeds:
            self.learned_network_profiles["thoughtseed_contributions"][ts] = NETWORK_PROFILES["thoughtseed_contributions"][ts].copy()

        for state in self.states:
            self.learned_network_profiles["state_network_expectations"][state] = NETWORK_PROFILES["state_expected_profiles"][state][self.experience_level].copy()
            
    def compute_network_activations(self, thoughtseed_activations, current_state, meta_awareness):
        """
        Compute attentional network activations based on current thoughtseed activations
        using bidirectional influences between networks and thoughtseeds.
        Implements Equation 1 from the Methods section.        
        """
        # Get thoughtseed-to-network contribution matrix
        ts_to_network = self.learned_network_profiles["thoughtseed_contributions"]
        
        # Initialize with baseline activations (prevents zeros)
        network_acts = {net: 0.2 for net in self.networks}
        
        # Bottom-up and top-down weights vary by experience level
        # First term of Equation 1: (1-ζ)∑_i W_{ik}z_i(t)
        if self.experience_level == 'expert':
            bottom_up_weight = 0.5  # This is ζ in the equation
            top_down_weight = 0.5   # This is (1-ζ) in the equation
        else:
            bottom_up_weight = 0.6  # More bottom-up for novices (more stimulus-driven) 
            top_down_weight = 0.4   # Less top-down for novices
        
        # Calculate bottom-up influence: thoughtseeds -> networks
        for i, ts in enumerate(self.thoughtseeds):
            ts_act = thoughtseed_activations[i]
            for net in self.networks:
                # Weighted contribution based on thoughtseed activation
                network_acts[net] += ts_act * ts_to_network[ts][net] * bottom_up_weight
    
        # Calculate top-down influence: state expectations -> networks
        # Second term of Equation 1: ζ μ_k(s_t)
        state_expect = self.learned_network_profiles["state_network_expectations"][current_state]
        for net in self.networks:
            # Meta-awareness amplifies top-down control (stronger for experts)
            meta_factor = meta_awareness * (1.2 if self.experience_level == 'expert' else 1.0)
            state_influence = state_expect[net] * meta_factor * top_down_weight
            network_acts[net] = (1 - top_down_weight) * network_acts[net] + state_influence
          
        # STATE-SPECIFIC ADJUSTMENTS WITH EXPERIENCE-DEPENDENT MODULATION
        
        # 1. META-AWARENESS STATE MODULATION
        if current_state == "meta_awareness":
            # Get modulation effects from configuration
            van_boost_effect = self.network_modulation.get('van_boost', {}).get('meta_awareness')
            fpn_boost_effect = self.network_modulation.get('fpn_boost', {}).get('meta_awareness')
            
            # Apply VAN boost - should be higher for experts (better salience detection)
            if van_boost_effect:
                van_boost = van_boost_effect.get_value(current_state, self.experience_level)
                # Experts get additional VAN boost from meta-awareness
                if self.experience_level == 'expert':
                    van_boost *= 1.5  # Stronger VAN activation during meta-awareness for experts
                network_acts['VAN'] += van_boost * meta_awareness
            
            # Apply FPN boost - more strategic control for experts
            if fpn_boost_effect:
                fpn_boost = fpn_boost_effect.get_value(current_state, self.experience_level)
                network_acts['FPN'] += fpn_boost * meta_awareness
            
            # DMN suppression during meta-awareness (stronger for experts)
            dmn_suppression_effect = self.network_modulation.get('dmn_suppression', {}).get('meta_awareness')
            if dmn_suppression_effect:
                dmn_suppress = dmn_suppression_effect.get_value(current_state, self.experience_level)
                # Experts have stronger DMN suppression during meta-awareness
                if self.experience_level == 'expert':
                    dmn_suppress *= 1.5  # 50% stronger DMN suppression for experts
                network_acts['DMN'] *= (1.0 - dmn_suppress)
            
            # Ensure VAN > FPN relationship during meta-awareness (detecting shifts)
            if network_acts['FPN'] >= network_acts['VAN']:
                network_acts['VAN'] = network_acts['FPN'] + 0.1
            
            # CONSOLIDATED EXPERT DMN ADJUSTMENT - Aligning with meditation literature (Brewer et al., 2011)
            if self.experience_level == 'expert':
                # Get expected profile value (from learned profiles)
                expected_dmn = self.learned_network_profiles["state_network_expectations"][current_state]["DMN"]
                
                # Apply stronger adjustment toward literature target
                current_dmn = network_acts['DMN']
                target_dmn = 0.2  # Target value from literature
                
                # Weighted momemntum based averaging (30% current, 70% target)
                network_acts['DMN'] = 0.3 * current_dmn + 0.7 * target_dmn
                
                # Ensure DMN is lower than novice levels (critical pattern in literature)
                max_dmn = 0.25  # Maximum allowed DMN for experts in meta-awareness
                network_acts['DMN'] = min(network_acts['DMN'], max_dmn)
                
                # Ensure VAN doesn't go too high (cap at 0.9)
                network_acts['VAN'] = min(0.9, network_acts['VAN'])
                
                # Ensure proper FPN value relationship to VAN during meta-awareness
                if network_acts['FPN'] > network_acts['VAN']:
                    network_acts['FPN'] = network_acts['VAN'] * 0.9  # FPN slightly lower than VAN

        # 2. MIND WANDERING STATE MODULATION
        elif current_state == "mind_wandering":
            # DMN boost (stronger for novices)
            dmn_boost_effect = self.network_modulation.get('dmn_boost', {}).get('mind_wandering')
            if dmn_boost_effect:
                dmn_boost = dmn_boost_effect.get_value(current_state, self.experience_level)
                # Novices show more exaggerated DMN activation during mind wandering
                if self.experience_level == 'novice':
                    dmn_boost *= 1.3  # 30% stronger DMN boost for novices
                network_acts['DMN'] += dmn_boost
            
            # DAN suppression (stronger for novices)
            dan_suppression = self.network_modulation.get('dan_suppression', {}).get('mind_wandering')
            if dan_suppression:
                dan_suppress_value = dan_suppression.get_value(current_state, self.experience_level)
                # Novices have stronger DAN suppression (harder to maintain focus)
                if self.experience_level == 'novice':
                    dan_suppress_value *= 1.25  # 25% stronger suppression for novices
                network_acts['DAN'] *= (1.0 - dan_suppress_value)
            
        # 3. FOCUSED STATES MODULATION (breath_control, redirect_breath)
        elif current_state in ["breath_control", "redirect_breath"]:
            # DAN boost (stronger for experts)
            dan_boost_effect = self.network_modulation.get('dan_boost', {}).get(current_state)
            if dan_boost_effect:
                dan_boost = dan_boost_effect.get_value(current_state, self.experience_level)
                # Experts have stronger DAN activation during focused states
                if self.experience_level == 'expert':
                    dan_boost *= 1.3  # 30% stronger DAN boost for experts
                network_acts['DAN'] += dan_boost * meta_awareness
            
            # DMN suppression (much stronger for experts)
            dmn_suppression_effect = self.network_modulation.get('dmn_suppression', {}).get(current_state)
            if dmn_suppression_effect:
                dmn_suppress = dmn_suppression_effect.get_value(current_state, self.experience_level)
                # Experts have dramatically stronger DMN suppression during focus
                if self.experience_level == 'expert':
                    dmn_suppress *= 1.5  # 50% stronger DMN suppression for experts
                network_acts['DMN'] *= (1.0 - dmn_suppress)
        
        # 4. EXPERT-SPECIFIC NETWORK RELATIONSHIPS
        if self.experience_level == 'expert':
            # Experts have stronger top-down control from FPN
            fpn_influence = self.fpn_enhancement * 0.2
            for net in ['DMN', 'VAN', 'DAN']:
                network_acts[net] = (1.0 - fpn_influence) * network_acts[net] + fpn_influence * network_acts['FPN']
        
        # 5. DMN-DAN ANTICORRELATION (much stronger for experts)
        dan_dmn_effect = self.network_modulation.get('dan_dmn_anticorrelation')
        dmn_dan_effect = self.network_modulation.get('dmn_dan_anticorrelation')
        
        # Get anticorrelation strengths with experience-specific modulation
        dan_dmn_anticorr_strength = 0.18  # Default value
        dmn_dan_anticorr_strength = 0.15  # Default value
        
        if dan_dmn_effect:
            dan_dmn_anticorr_strength = dan_dmn_effect.get_value(current_state, self.experience_level)
            # Experts have stronger anticorrelation (better network separation)
            if self.experience_level == 'expert':
                dan_dmn_anticorr_strength *= 1.5  # 50% stronger for experts
        
        if dmn_dan_effect:
            dmn_dan_anticorr_strength = dmn_dan_effect.get_value(current_state, self.experience_level)
            if self.experience_level == 'expert':
                dmn_dan_anticorr_strength *= 1.5  # 50% stronger for experts
        
        # Get non-linear dynamics parameters
        boundary_threshold = self.non_linear_dynamics.anticorrelation_boundary_threshold
        
        # DMN anticorrelation with asymptotic approach
        anticorr_effect = dan_dmn_anticorr_strength * (network_acts['DAN'] - 0.5)
        # Apply diminishing effect near boundaries
        if anticorr_effect > 0 and network_acts['DMN'] < boundary_threshold:
            anticorr_effect *= (network_acts['DMN'] / boundary_threshold)
        elif anticorr_effect < 0 and network_acts['DMN'] > (1.0 - boundary_threshold):
            anticorr_effect *= (1.0 - (network_acts['DMN'] - (1.0 - boundary_threshold)) / boundary_threshold)
        network_acts['DMN'] = max(0.05, min(0.95, network_acts['DMN'] - anticorr_effect))
        
        # DAN anticorrelation with similar approach
        anticorr_effect = dmn_dan_anticorr_strength * (network_acts['DMN'] - 0.5)
        if anticorr_effect > 0 and network_acts['DAN'] < boundary_threshold:
            anticorr_effect *= (network_acts['DAN'] / boundary_threshold)
        elif anticorr_effect < 0 and network_acts['DAN'] > (1.0 - boundary_threshold):
            anticorr_effect *= (1.0 - (network_acts['DAN'] - (1.0 - boundary_threshold)) / boundary_threshold)
        network_acts['DAN'] = max(0.05, min(0.95, network_acts['DAN'] - anticorr_effect))
    
        # DMN/DAN-specific smoothing for all experience levels
        if len(self.network_activations_history) > 3:
            volatility_nets = ['DMN', 'DAN']
            dmn_dan_smoothing = 0.25 if self.experience_level == 'novice' else 0.35
            for net in volatility_nets:
                recent_values = [self.network_activations_history[-i][net] for i in range(1, min(4, len(self.network_activations_history)+1))]
                network_acts[net] = (1-dmn_dan_smoothing) * network_acts[net] + dmn_dan_smoothing * np.mean(recent_values)
                
        # Apply memory factor for temporal stability (μ parameter from document)
        if hasattr(self, 'prev_network_acts') and self.prev_network_acts:
            for net in self.networks:
                network_acts[net] = self.memory_factor * self.prev_network_acts[net] + (1 - self.memory_factor) * network_acts[net]
            
        # Apply non-linear compression to prevent extremes (sigmoid-like behavior)
        high_threshold = self.non_linear_dynamics.high_compression_threshold
        low_threshold = self.non_linear_dynamics.low_compression_threshold
        high_factor = self.non_linear_dynamics.high_compression_factor
        low_factor = self.non_linear_dynamics.low_compression_factor
        
        for net in ['DMN', 'DAN']:
            if network_acts[net] > high_threshold:
                # Compress high values (0.8-1.0 range gets compressed)
                compression = (network_acts[net] - high_threshold) * high_factor
                network_acts[net] = high_threshold + compression
            elif network_acts[net] < low_threshold:
                # Compress low values (0-0.2 range gets compressed)
                compression = (low_threshold - network_acts[net]) * low_factor
                network_acts[net] = low_threshold - compression

        # Enhanced smoothing for specific conditions
        if current_state == "mind_wandering" and self.experience_level == 'expert':
            # Prevent extreme DMN activation during mind wandering
            max_dmn = self.non_linear_dynamics.max_dmn_mind_wandering
            network_acts['DMN'] = min(network_acts['DMN'], max_dmn)
            
            # Apply stronger temporal smoothing when network activations are changing rapidly
            if len(self.network_activations_history) > 3:
                rapid_change_threshold = self.non_linear_dynamics.rapid_change_threshold
                rapid_change_smoothing = self.non_linear_dynamics.rapid_change_smoothing
                
                for net in ['DMN', 'DAN']:
                    recent = self.network_activations_history[-1][net]
                    current = network_acts[net]
                    # If rapid change detected
                    if abs(current - recent) > rapid_change_threshold:
                        # Apply stronger smoothing
                        network_acts[net] = rapid_change_smoothing * recent + (1.0 - rapid_change_smoothing) * current
                        
        # Normalize and add noise
        for net in self.networks:
            network_acts[net] = np.clip(network_acts[net], 0.05, 1.0)
            # Add small noise for biological plausibility
            network_acts[net] += np.random.normal(0, self.noise_level)
            network_acts[net] = np.clip(network_acts[net], 0.05, 1.0)
            
        # VAN values > 0.85 are neurophysiologically implausible
        max_van = 0.85
        if network_acts['VAN'] > max_van:
            network_acts['VAN'] = max_van
            
        # During meta-awareness, ensure key network relationships for experts
        if current_state == "meta_awareness" and self.experience_level == 'expert':
            # Ensure FPN doesn't exceed VAN (literature shows VAN activation precedes FPN)
            if network_acts['FPN'] > network_acts['VAN']:
                network_acts['FPN'] = network_acts['VAN'] * 0.95
            
            # Ensure DMN-DAN anticorrelation is maintained
            if network_acts['DMN'] > 0.3 and network_acts['DAN'] > 0.5:
                # Reduce DMN to maintain anticorrelation pattern
                network_acts['DMN'] *= 0.85
        
        return network_acts
    
    def calculate_free_energy(self, network_acts, current_state, meta_awareness):
        """
        Calculate variational free energy as prediction error plus complexity cost.
        
        Implements Equation 2 from the theoretical framework:
        F_t(s) = ∑_k Π_k(ψ_t)[n_k(t) - μ_k(s_t)]² + λ ||W||_F²
        
        Where:
        - First term: Precision-weighted prediction error (accuracy)
        - Second term: Complexity penalty on the mapping matrix (parsimony)
        - Π_k(ψ_t): Precision that increases with meta-awareness
        - λ: Complexity penalty (lower for experts, higher for novices)
        """
        # Get expected network profile for current state
        expected_profile = self.learned_network_profiles["state_network_expectations"][current_state]
        
        # Calculate prediction error (squared difference)
        prediction_errors = {}
        total_prediction_error = 0
        
        for net in self.networks:
            # Calculate error between actual and expected - implements [n_k(t) - μ_k(s_t)]²
            error = (network_acts[net] - expected_profile[net])**2
            
            # State-specific error adjustments
            if current_state == "mind_wandering" and net == "DMN":
                if self.experience_level == 'expert':
                    # Experts have lower prediction error for DMN during mind-wandering
                    error *= 0.8
                else:
                    # Novices have higher prediction error for DMN during mind-wandering
                    error *= 1.2
            
            prediction_errors[net] = error
            
            # Weight error by precision (higher meta-awareness = higher precision)
            # This implements Π_k(ψ_t) scaling with meta-awareness
            precision = 0.5 + self.precision_weight * meta_awareness
            weighted_error = error * precision
            total_prediction_error += weighted_error
        
        # Calculate complexity cost - higher in novices, lower in experts
        # This implements λ ||W||_F² term (complexity penalty) in Eq 2
        complexity_cost = self.complexity_penalty
        
        # Free energy = prediction error + complexity cost
        # This completes the implementation of Equation 2
        free_energy = total_prediction_error + complexity_cost
        
        return free_energy, prediction_errors, total_prediction_error
    
    def update_network_profiles(self, thoughtseed_activations, network_activations, current_state, prediction_errors):
        """
        Update learned network profiles based on prediction errors.
        
        Implements Equation 3 from the theoretical framework:
        W_ik ← (1-ρ)W_ik + η δ_k(t)z_i(t)
        
        Where:
        - δ_k(t) = n_k(t) - μ_k(s_t): Prediction error for network k
        - η: Learning rate (higher for experts)
        - ρ: Weight decay factor to prevent runaway growth
        - z_i(t): Activation of thoughtseed i
        """
        # Only update after some initial observations
        if len(self.network_activations_history) < 10:
            return
        
        # For each thoughtseed contribution to networks
        for i, ts in enumerate(self.thoughtseeds):
            ts_act = thoughtseed_activations[i]  # This is z_i(t) in Equation 3
            
            # Only update when thoughtseed is significantly active
            if ts_act > 0.2:
                for net in self.networks:
                    # Current prediction and error
                    current_contrib = self.learned_network_profiles["thoughtseed_contributions"][ts][net]
                    current_error = prediction_errors[net] # Related to δ_k(t) in Equation 3
                    
                    # Calculate precision (confidence) - higher for experts
                    precision = 1.0 + (5.0 if self.experience_level == 'expert' else 2.0) * len(self.network_activations_history)/self.timesteps
                    
                    # Bayesian-inspired update (approximating Equation 3)
                    # The error_sign term adapts the direction of weight updates based on current state expectations
                    error_sign = 1 if network_activations[net] < self.learned_network_profiles["state_network_expectations"][current_state][net] else -1
                    update = self.learning_rate * (error_sign * current_error) * ts_act / precision
                    
                    # Update contribution - implements W_ik ← W_ik + update term. The (1-ρ) decay factor is implicitly applied through normalization and bounds
                    self.learned_network_profiles["thoughtseed_contributions"][ts][net] += update
                    
                    # Ensure biological plausibility by bounding weights
                    # This restricts weights to [W_min, W_max] range as mentioned in the document
                    self.learned_network_profiles["thoughtseed_contributions"][ts][net] = np.clip(
                        self.learned_network_profiles["thoughtseed_contributions"][ts][net], 0.1, 0.9)
        
        # Update state network expectations (slower learning rate)
        slow_rate = self.learning_rate * 0.3
        for net in self.networks:
            # Moving average update
            current_expect = self.learned_network_profiles["state_network_expectations"][current_state][net]
            new_value = (1 - slow_rate) * current_expect + slow_rate * network_activations[net]
            self.learned_network_profiles["state_network_expectations"][current_state][net] = new_value
    
    def network_modulated_activations(self, activations, network_acts, current_state):
        """Apply network-based modulation to thoughtseed activations"""
        modulated_acts = activations.copy()
        
        # Apply network-specific effects
        
        # DMN enhances pending_tasks and self_reflection, suppresses breath_focus
        dmn_strength = network_acts['DMN']
        
        # Get DMN enhancement effects
        dmn_pending_effect = self.network_modulation.get('dmn_pending_enhancement')
        dmn_reflection_effect = self.network_modulation.get('dmn_reflection_enhancement')
        dmn_breath_effect = self.network_modulation.get('dmn_breath_suppression')
        
        # Use default values if effects are not found
        dmn_pending_value = dmn_pending_effect.get_value(current_state, self.experience_level) if dmn_pending_effect else 0.15
        dmn_reflection_value = dmn_reflection_effect.get_value(current_state, self.experience_level) if dmn_reflection_effect else 0.1
        dmn_breath_value = dmn_breath_effect.get_value(current_state, self.experience_level) if dmn_breath_effect else 0.2
        
        modulated_acts[self.thoughtseeds.index('pending_tasks')] += dmn_pending_value * dmn_strength
        modulated_acts[self.thoughtseeds.index('self_reflection')] += dmn_reflection_value * dmn_strength
        modulated_acts[self.thoughtseeds.index('breath_focus')] -= dmn_breath_value * dmn_strength

        # VAN enhances pain_discomfort (salience) and self_reflection during meta_awareness
        van_strength = network_acts['VAN']
        
        # Get VAN enhancement effects
        van_pain_effect = self.network_modulation.get('van_pain_enhancement')
        van_pain_value = van_pain_effect.get_value(current_state, self.experience_level) if van_pain_effect else 0.15
        
        modulated_acts[self.thoughtseeds.index('pain_discomfort')] += van_pain_value * van_strength
        
        # VAN also enhances self-reflection during meta-awareness (critical for Vipassana)
        if current_state == "meta_awareness":
            van_reflection_effect = self.network_modulation.get('van_reflection_enhancement')
            van_reflection_value = van_reflection_effect.get_value(current_state, self.experience_level) if van_reflection_effect else 0.2
            modulated_acts[self.thoughtseeds.index('self_reflection')] += van_reflection_value * van_strength

        # DAN enhances breath_focus, suppresses distractions
        dan_strength = network_acts['DAN'] 
        
        # Get DAN enhancement and suppression effects
        dan_breath_effect = self.network_modulation.get('dan_breath_enhancement')
        dan_pending_effect = self.network_modulation.get('dan_pending_suppression')
        dan_pain_effect = self.network_modulation.get('dan_pain_suppression')
        
        dan_breath_value = dan_breath_effect.get_value(current_state, self.experience_level) if dan_breath_effect else 0.2
        dan_pending_value = dan_pending_effect.get_value(current_state, self.experience_level) if dan_pending_effect else 0.15
        dan_pain_value = dan_pain_effect.get_value(current_state, self.experience_level) if dan_pain_effect else 0.1
        
        modulated_acts[self.thoughtseeds.index('breath_focus')] += dan_breath_value * dan_strength
        modulated_acts[self.thoughtseeds.index('pending_tasks')] -= dan_pending_value * dan_strength
        modulated_acts[self.thoughtseeds.index('pain_discomfort')] -= dan_pain_value * dan_strength
        
        # FPN enhances self_reflection and equanimity (metacognition and regulation)
        fpn_strength = network_acts['FPN']
        fpn_enhancement = self.fpn_enhancement  # Should be 1.0 for novice, 1.2 for expert
        
        # Get FPN enhancement effects
        fpn_reflection_effect = self.network_modulation.get('fpn_reflection_enhancement')
        fpn_equanimity_effect = self.network_modulation.get('fpn_equanimity_enhancement')
        
        default_reflection = 0.2 if self.experience_level == 'expert' else 0.15
        default_equanimity = 0.25 if self.experience_level == 'expert' else 0.2
        
        fpn_reflection_value = fpn_reflection_effect.get_value(current_state, self.experience_level) if fpn_reflection_effect else default_reflection
        fpn_equanimity_value = fpn_equanimity_effect.get_value(current_state, self.experience_level) if fpn_equanimity_effect else default_equanimity
        
        if self.experience_level == 'expert':
            modulated_acts[self.thoughtseeds.index('self_reflection')] += fpn_reflection_value * fpn_strength * fpn_enhancement
            modulated_acts[self.thoughtseeds.index('equanimity')] += fpn_equanimity_value * fpn_strength * fpn_enhancement
        else:
            modulated_acts[self.thoughtseeds.index('self_reflection')] += fpn_reflection_value * fpn_strength
            modulated_acts[self.thoughtseeds.index('equanimity')] += fpn_equanimity_value * fpn_strength
                
        # Normalize to prevent extreme values
        modulated_acts = np.clip(modulated_acts, 0.05, 1.0)
        
        return modulated_acts
    
    def get_meta_awareness(self, current_state, activations):
        """Calculate meta-awareness using the base implementation"""
        # Uses MetacognitionParams
        return super().get_meta_awareness(current_state, activations)
    
    def get_dwell_time(self, state):
        """Get state-specific dwell time based on experience level."""
        # Use parent method 
        return super().get_dwell_time(state)
    
    def train(self):
        """
        Train the model using active inference principles:
        1. Calculate network activations
        2. Compute free energy (prediction errors)
        3. Update network profiles based on prediction errors
        4. Use free energy to influence state transitions
        """
        # Create directories for output
        ensure_directories()
        
        # Initialize training sequence similar to base class
        state_sequence = ["breath_control", "mind_wandering", "meta_awareness", "redirect_breath"]
        current_state_index = 0
        current_state = state_sequence[current_state_index]
        current_dwell = 0
        dwell_limit = self.get_dwell_time(current_state)
        
        # Initialize activations
        activations = np.full(self.num_thoughtseeds, np.random.uniform(0.05, 0.15))
        activations = self.get_target_activations(current_state, 0.6)
        prev_activations = activations.copy()
        
        # Track focused state timing as in base class
        time_in_focused_state = 0
        state_transition_patterns = []
        transition_timestamps = []
        
        # Initialize network tracking
        network_acts = self.compute_network_activations(activations, current_state, 0.6)
        prev_network_acts = network_acts.copy()
        
        # Training loop
        for t in range(self.timesteps):
            # Calculate meta-awareness as in base class
            meta_awareness = self.get_meta_awareness(current_state, activations)
            
            if hasattr(self, 'in_transition') and self.in_transition:
                # Continue smoothing the transition over multiple timesteps with variability
                blend_factor = 0.3 * (1.0 + np.random.uniform(-0.1, 0.1))  # 27-33% blend
                
                # Add small random perturbations to transition target
                perturbed_target = self.transition_target.copy()
                perturbed_target += np.random.normal(0, 0.02, size=len(perturbed_target))
                perturbed_target = np.clip(perturbed_target, 0.05, 1.0)
                
                # Apply blending with perturbed target
                activations = (1 - blend_factor) * activations + blend_factor * perturbed_target
                
                # Decrement counter and check if transition is complete
                self.transition_counter -= 1
                if self.transition_counter <= 0:
                    self.in_transition = False

            # Get target activations (from base class)
            target_activations = self.get_target_activations(current_state, meta_awareness)
            
            # Smooth activation transitions as in base class
            if current_dwell < 3:
                alpha = (current_dwell + 1) / 3
                activations = (1 - alpha) * prev_activations + alpha * target_activations * 0.9 + prev_activations * 0.1
            else:
                activations = target_activations * 0.9 + prev_activations * 0.1
            
            # Apply state-specific adjustments from base class
            if current_state == "mind_wandering" and meta_awareness < 0.6:
                for ts in self.thoughtseeds:
                    i = self.thoughtseeds.index(ts)
                    if ts == "breath_focus":
                        # Much more biological variability for suppressed attention
                        base_level = max(0.05, activations[i] * 0.3)  # Increased from 0.2
                        # Larger noise component based on time in state
                        noise_amplitude = 0.03 + 0.01 * min(1.0, current_dwell/10)
                        activations[i] = base_level + np.random.normal(0, noise_amplitude)
                    elif ts in ["pain_discomfort", "pending_tasks"]:
                        # Add fluctuations to dominant thoughtseeds
                        growth_factor = 1.2 * (1.0 + np.random.uniform(-0.1, 0.1))
                        activations[i] *= growth_factor
                    else:
                        # More variable suppression for other thoughtseeds
                        suppress_factor = 0.5 * (1.0 + np.random.uniform(-0.15, 0.15))
                        activations[i] *= suppress_factor
            elif current_state == "meta_awareness" and meta_awareness >= 0.8:
                for ts in self.thoughtseeds:
                    i = self.thoughtseeds.index(ts)
                    if ts == "self_reflection":
                        activations[i] *= 1.5
                    else:
                        activations[i] *= 0.2
            elif current_state == "redirect_breath" and meta_awareness >= 0.8:
                for ts in self.thoughtseeds:
                    i = self.thoughtseeds.index(ts)
                    if ts == "equanimity":
                        activations[i] *= 1.5
                    elif ts == "breath_focus":
                        activations[i] *= 1.1
                    else:
                        activations[i] *= 0.3
                        
            # Enhanced mind-wandering for novices with dominant pending tasks
            if current_state == "mind_wandering" and self.experience_level == 'novice':
                # Make pending_tasks more dominant and persistent
                pt_idx = self.thoughtseeds.index("pending_tasks")
                # Boost based on current level (the higher it is, the more it grows)
                activations[pt_idx] *= 1.15
                
                # Create interference with self-reflection
                sr_idx = self.thoughtseeds.index("self_reflection")
                # Pending tasks interferes with self-reflection (making meta-awareness harder)
                interference = min(0.3, activations[pt_idx] * 0.4)
                activations[sr_idx] = max(0.05, activations[sr_idx] - interference)
                
                # This also affects network dynamics
                # Higher pending_tasks activation strengthens DMN
                network_acts['DMN'] += activations[pt_idx] * 0.15
                # And suppresses FPN (making meta-awareness harder)
                network_acts['FPN'] = max(0.05, network_acts['FPN'] - activations[pt_idx] * 0.15)
            
            # Handle distraction growth in focused states (from base class)
            if current_state in ["breath_control", "redirect_breath"]:
                time_in_focused_state += 1
                dwell_factor = min(1.0, current_dwell / max(10, dwell_limit))
                
                distraction_scale = 2.5 if self.experience_level == 'novice' else 1.2
                distraction_growth = 0.035 * dwell_factor * distraction_scale
                
                self.distraction_buildup_rates.append(distraction_growth)
                
                boost_chance = 0.1
                boost_factor = 1.0
                if np.random.random() < boost_chance:
                    boost_factor = 3.0
                
                for i, ts in enumerate(self.thoughtseeds):
                    if ts in ["pain_discomfort", "pending_tasks"]:
                        activations[i] += distraction_growth * boost_factor
                
                for i, ts in enumerate(self.thoughtseeds):
                    if ts == "breath_focus":
                        fatigue_rate = 0.005 if self.experience_level == 'expert' else 0.01
                        fatigue = fatigue_rate * dwell_factor * time_in_focused_state/10
                        activations[i] = max(0.2, activations[i] - fatigue)
            else:
                time_in_focused_state = 0
            
            # Cap extreme values
            for i, ts in enumerate(self.thoughtseeds):
                if ts == "pending_tasks" and activations[i] > 0.8:
                    activations[i] = 0.8
            
            # Expert-specific adjustments from base class
            if self.experience_level == 'expert' and current_state in ["redirect_breath", "meta_awareness"]:
                bf_idx = self.thoughtseeds.index("breath_focus")
                eq_idx = self.thoughtseeds.index("equanimity")
                
                if activations[bf_idx] > 0.3 and activations[eq_idx] > 0.3:
                    boost = 0.03 * min(activations[bf_idx], activations[eq_idx])
                    activations[bf_idx] += boost
                    activations[eq_idx] += boost
                
                if activations[eq_idx] > 0.4:
                    pd_idx = self.thoughtseeds.index("pain_discomfort")
                    activations[pd_idx] = max(0.05, activations[pd_idx] - 0.02 * activations[eq_idx])
            
            # Expert-specific adjustments from base class
            if self.experience_level == 'expert' and current_state in ["breath_control", "redirect_breath"]:
                bf_idx = self.thoughtseeds.index("breath_focus")
                eq_idx = self.thoughtseeds.index("equanimity")
                
                if current_state == "breath_control" and current_dwell < dwell_limit * 0.3:
                    # Lower initial equanimity in breath_control 
                    activations[eq_idx] *= 0.85
                elif activations[bf_idx] > 0.4:
                    # Breath focus facilitates equanimity 
                    facilitation = 0.08 * activations[bf_idx]
                    activations[eq_idx] += facilitation * (1.0 + np.random.uniform(-0.2, 0.2))
                    activations[eq_idx] = min(1.0, activations[eq_idx])
                    
            # Add physiological noise to all activations for biological plausibility
            for i, ts in enumerate(self.thoughtseeds):
                # Base noise level 
                noise_level = 0.005
                
                # More noise during mind_wandering to avoid fixed values
                if current_state == "mind_wandering":
                    noise_level = 0.015
                
                # Different thoughtseeds have different noise characteristics
                if ts in ["breath_focus", "equanimity"]:
                    # More stable attentional focus has less noise
                    noise_level *= 0.8
                elif ts in ["pain_discomfort"]:
                    # Pain fluctuates more
                    noise_level *= 1.5
                
                # Apply the noise
                activations[i] += np.random.normal(0, noise_level)
            
            # Ensure valid range
            activations = np.clip(activations, 0.05, 1.0)
                        
            # Compute network activations
            network_acts = self.compute_network_activations(activations, current_state, meta_awareness)
            
            # Calculate free energy (prediction error)
            free_energy, prediction_errors, total_prediction_error = self.calculate_free_energy(
                network_acts, current_state, meta_awareness)
            
            # Update network profiles based on prediction errors
            self.update_network_profiles(activations, network_acts, current_state, prediction_errors)
            
            # Apply network-based modulation to thoughtseed activations
            activations = self.network_modulated_activations(activations, network_acts, current_state)
            
            # Record network state and free energy for analysis
            self.network_activations_history.append(network_acts.copy())
            self.free_energy_history.append(free_energy)
            self.prediction_error_history.append(total_prediction_error)
            self.precision_history.append(0.5 + self.precision_weight * meta_awareness)
            
            # Identify dominant thoughtseed
            dominant_ts = self.thoughtseeds[np.argmax(activations)]
            
            # Track histories (base class)
            self.state_history.append(current_state)
            self.activations_history.append(activations.copy())
            self.meta_awareness_history.append(meta_awareness)
            self.dominant_ts_history.append(dominant_ts)
            self.state_history_over_time.append(self.state_indices[current_state])
            
            # Handle state transitions
            if current_dwell >= dwell_limit:
                # Save the activation pattern that led to this transition
                self.transition_activations[current_state].append(activations.copy())
                
                # Allow natural transitions based on activation patterns
                natural_transition = False
                next_state = None
                
                # IMPROVED: Transition probability influenced by free energy and training progress
                # Higher base probability for more natural transitions
                natural_prob = 0.8 + min(0.15, t / self.timesteps * 0.2)
                
                # Higher free energy increases transition probability
                # Systems with high prediction error are more likely to transition
                # This implements the concept from Equation 4 where transitions minimize free energy
                precision_factor = 1.5 if self.experience_level == 'expert' else 0.8
                fe_factor = min(0.3, free_energy * 0.3 * precision_factor)
                natural_prob = min(0.95, natural_prob + fe_factor)  
                
                
                # This section implements P(s_{t+1}=s') based on Equation 4, which 
                # specifies that the next state is selected to minimize free energy
                # subject to threshold conditions Θ(s_t → s')                
                if np.random.random() < natural_prob:                    
                    # FOCUSED STATES TO MIND WANDERING
                    if current_state in ["breath_control", "redirect_breath"]:
                        # Calculate combined distraction level
                        distraction_level = activations[self.thoughtseeds.index("pain_discomfort")] + \
                                            activations[self.thoughtseeds.index("pending_tasks")]
                        
                        # Consider DMN/DAN ratio but with lower threshold
                        dmn_dan_ratio = network_acts['DMN'] / (network_acts['DAN'] + 0.1)
                        
                        # Either high distraction OR high DMN/DAN ratio can trigger transition
                        if distraction_level > self.transition_thresholds['mind_wandering'] or dmn_dan_ratio > self.transition_thresholds['dmn_dan_ratio']:
                            next_state = "mind_wandering"
                            natural_transition = True
                    
                    # MIND WANDERING TO META-AWARENESS
                    elif current_state == "mind_wandering":
                        # Self-reflection is the key factor
                        self_reflection = activations[self.thoughtseeds.index("self_reflection")]
                        
                        # Consider VAN activation as secondary factor
                        van_activation = network_acts['VAN']
                        
                        # Simplified check with more accessible thresholds
                        awareness_threshold = 0.35 if self.experience_level == 'expert' else 0.45
                        
                        if self_reflection > awareness_threshold or \
                        (van_activation > 0.4 and self_reflection > 0.3):
                            next_state = "meta_awareness"
                            natural_transition = True
                    
                    # META-AWARENESS TO FOCUSED STATES
                    elif current_state == "meta_awareness":
                        # Base transition values (from activations)
                        bf_value = activations[self.thoughtseeds.index("breath_focus")]
                        eq_value = activations[self.thoughtseeds.index("equanimity")]
                        
                        # Network influences on transitions
                        # DAN activation promotes breath focus
                        bf_value += network_acts['DAN'] * 0.2
                        
                        # FPN activation promotes equanimity 
                        eq_value += network_acts['FPN'] * 0.2
                        
                        # Lower threshold for more reliable transitions
                        threshold = self.transition_thresholds['return_focus']
                        
                        if bf_value > threshold and eq_value > threshold:
                            # If both are high, experts favor equanimity/redirect_breath, novices favor breath_control
                            if self.experience_level == 'expert' and eq_value > bf_value:
                                next_state = "redirect_breath"
                            else:
                                next_state = "breath_control"
                            natural_transition = True
                        elif bf_value > threshold + 0.1:  # Higher certainty for single condition
                            next_state = "breath_control"
                            natural_transition = True
                        elif eq_value > threshold + 0.1:  # Higher certainty for single condition
                            next_state = "redirect_breath"
                            natural_transition = True
                
                # Follow fixed sequence if no natural transition
                if not natural_transition:
                    next_state_index = (current_state_index + 1) % len(state_sequence)
                    next_state = state_sequence[next_state_index]
                    self.forced_transition_count += 1
                else:
                    # This records naturally occurring transitions driven by free energy minimization
                    # as specified in Equation 4
                    self.natural_transition_count += 1
                    transition_timestamps.append(t)
                    state_transition_patterns.append((
                        current_state, 
                        next_state, 
                        {ts: activations[i] for i, ts in enumerate(self.thoughtseeds)},
                        {net: val for net, val in network_acts.items()},
                        free_energy
                    ))
                
                # Record the transition
                self.transition_counts[current_state][next_state] += 1
                
                # Update state
                current_state_index = state_sequence.index(next_state)
                current_state = next_state
                current_dwell = 0
                dwell_limit = self.get_dwell_time(current_state)

                # More gradual transition with biological variability
                new_target = self.get_target_activations(current_state, meta_awareness)

                # Add variability to target activations for biological plausibility
                for i in range(len(new_target)):
                    # 5-10% random variation in targets
                    variation = 1.0 + np.random.uniform(-0.05, 0.1)
                    new_target[i] *= variation
                    # Ensure we don't go below minimum
                    new_target[i] = max(0.06, new_target[i])  # Slightly higher min for targets

                # More conservative blending with variability
                blend_factor = 0.4 * (1.0 + np.random.uniform(-0.1, 0.1))  # 36-44% blend
                activations = (1 - blend_factor) * activations + blend_factor * new_target

                # Add transition markers to allow for continued smoothing in subsequent timesteps
                self.in_transition = True
                self.transition_counter = 3 + np.random.randint(0, 2)  # Variable transition time
                self.transition_target = new_target.copy()

            else:
                current_dwell += 1
            
            # Store for next iteration
            prev_activations = activations.copy()
            prev_network_acts = network_acts.copy()
        
        # Handle fallback for minimum natural transitions
        if self.natural_transition_count < 4:  # Require at least 4 natural transitions
            print(f"WARNING: Only {self.natural_transition_count} natural transitions occurred. Adding additional natural transitions...")
            # Add more natural transitions if needed (but don't completely overwrite existing ones)
            for i, state in enumerate(self.states):
                next_state = self.states[(i + 1) % len(self.states)]
                # Only add if this transition isn't already well-represented
                if self.transition_counts[state][next_state] < 2:
                    self.transition_counts[state][next_state] += 1
                    self.natural_transition_count += 1
        
        # Save learned weights and network profiles
        ensure_directories('./results_act_inf')
        
        # Save transition statistics with network data
        transition_stats = {
            'transition_counts': self.transition_counts,
            'transition_thresholds': self.transition_thresholds,  # Use configured thresholds
            'natural_transitions': self.natural_transition_count,
            'forced_transitions': self.forced_transition_count,
            'transition_timestamps': transition_timestamps,
            'state_transition_patterns': state_transition_patterns,
            'distraction_buildup_rates': self.distraction_buildup_rates,
            'average_activations_at_transition': {
                state: np.mean(acts, axis=0).tolist() if len(acts) > 0 else np.zeros(self.num_thoughtseeds).tolist()
                for state, acts in self.transition_activations.items()
            },
            'average_network_activations_by_state': {
                state: {
                    net: float(np.mean([
                        self.network_activations_history[j][net]
                        for j, s in enumerate(self.state_history) if s == state
                    ])) for net in self.networks
                } for state in self.states if any(s == state for s in self.state_history)
            },
            'average_free_energy_by_state': {
                state: float(np.mean([
                    self.free_energy_history[j]
                    for j, s in enumerate(self.state_history) if s == state
                ])) for state in self.states if any(s == state for s in self.state_history)
            }
        }
        
        # Debug
        print(f"\n{self.experience_level.upper()} NETWORK VALUES BY STATE:")
        for state in self.states:
            print(f"  {state}:")
            state_networks = {
                net: float(np.mean([
                    self.network_activations_history[j][net]
                    for j, s in enumerate(self.state_history) if s == state
                ])) for net in self.networks
            }
            for net in self.networks:
                print(f"    {net}: {state_networks[net]:.2f}")

        # Save the transition statistics
        with open(f"./results_act_inf/data/transition_stats_{self.experience_level}.json", "w") as f:
            json.dump(transition_stats, f, indent=2)

        print(f"Active Inference training complete for {self.experience_level}.")
        print(f"  - Natural transitions: {self.natural_transition_count}, Forced transitions: {self.forced_transition_count}")

        # Generate JSON outputs (time series data and parameters)
        _save_json_outputs(self)
